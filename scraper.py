from pathlib import Path
from bs4 import BeautifulSoup

import argparse
import datetime
import os
import requests

BASE_URL = 'https://kitsunekko.net/'
TOPLEVEL_DIR = 'dirlist.php?dir=subtitles%2Fjapanese%2F'
DEFAULT_TOPLEVEL_FILENAME = 'kitsunekko.html'

subs_dir = './data/'
temp_pages_dir = './pages/'
temp_subpages_dir = 'subpage/'

# Set up argument parser.
argParser = argparse.ArgumentParser(description='Scrapes Kitsunekko.net for Japanese subtitles.')

argParser.add_argument('--force-dl', 
    action = 'store_true',
    help = 'Whether or not to re-download files you already have.'
)

argParser.add_argument('--override-dir', 
    action = 'store', 
    type=str,
    help='Pass in a string to replace the default directory to download subtitles (default ./data).'
)

argParser.add_argument('--use-local', 
    action = 'store_true',
    help='Whether to fetch data from the remote page or from a local copy. Mostly for Debug purposes.'
)

args = argParser.parse_args()

if(args.override_dir):
    subs_dir = args.override_dir
    if(not subs_dir.endswith('/')):
        subs_dir = subs_dir + '/'
    print("User overrided directory: " + subs_dir)

if(not os.path.exists(Path(subs_dir))):
    os.makedirs(Path(subs_dir))

## Scrapes the primary Kitsunekko page and writes it to a file.
## @param url: The url to scrape (kitsunekko.net).
def scrape_main(url):
    # Configure a temp file.
    mainPagePath = Path(temp_pages_dir + DEFAULT_TOPLEVEL_FILENAME)

    # If the user doesn't already have a pages dir, make it.
    if(not os.path.exists(Path(temp_pages_dir)) ):
        os.makedirs(Path(temp_pages_dir))

    # If the user doesn't have a pre-existing page to chew on, or they didn't set use-local, write it.
    if(not os.path.isfile(mainPagePath) or not args.use_local):
        print("Downloading the main kitsunekko.net page.")
        data = requests.get( (BASE_URL + TOPLEVEL_DIR) ) 
        data = data.text
        
        file = open(mainPagePath, "w")
        file.write(data)
        file.close()

## Scrapes all subpages from a top level page.
## @param page: Page to scrape for data.
## @returns a constrcuted list to the paths of all of the fetched pages.
def fetch_subpages(pageList, session):
    page_list = []
    for page, name in pageList:
        if(page.endswith('%2F') or page.endswith('%2f')):
            subPagePath = Path(temp_pages_dir + temp_subpages_dir + page + ".html")
            page_list.append( ( (temp_subpages_dir + page + ".html"), name) )

            # Create the path if it doesn't exist already.
            if(not os.path.exists(temp_pages_dir + temp_subpages_dir)):
                os.makedirs(temp_pages_dir + temp_subpages_dir)

            # If the page we're looking to scrape hasn't been written already, or the user hasn't asked to use local files, then go download it.
            if(not os.path.isfile(subPagePath) or not args.use_local):
                print("Downloading Page " + name)
                data = session.get( (BASE_URL + page) )
                data = data.text
                
                file = open(subPagePath, 'w')
                file.write(data)
                file.close()
            else:
                print("Using local versoin of page " + name)
    return page_list

## Given a page to eat, scrapes for the first table body found and creates a list of tuples for the href and the value.
## @param  page (optional) The filename of the page to read
## @returns A list of tuples, in the format (url stem, name of group) 
def generate_arr(*, page = DEFAULT_TOPLEVEL_FILENAME):
    # Open the DL'd or existing page to read.
    pagePath = Path(temp_pages_dir + page)
    file = open(pagePath, 'r')
    data = file.read()

    # Create our BS4 object and generate a list of individual pages.
    htmlParse = BeautifulSoup(data, 'html.parser')
    
    # The main page only has one table; We could make this more robust in the future.
    anitable = htmlParse.body.table.tbody
    links = anitable.find_all('a')
    individual_links = []
    for link in links:
        individual_links.append((link['href'], link.strong.text))

    return individual_links

## Given a list of tuples generated by generate_arr, iterates though and downloads all attached pages and subs.
## If it finds a sub-page under the listed URL, it will recurse and pass that name down for the children to exist in that subfolder.
## @param urlList: a list of tuples in the format (url stem, name) from generate_arr().
## @param (optional) parentDir: the directory in which this file should live.
def download_subs(urlList, session, *, parentDir = None):
    for page, name in urlList:
        # If the page url ends with %2F, we have another page to traverse.
        # Recurse down the page.
        # Otherwise, we've found a file, so go ahead and download it.
        # TODO: A little smelly. I'd like to cut back on duplicate code later.
        # TODO: I'd also like to make these asynchronous so that we are not blocked by request slowly downloading.
        if(parentDir is not None):
            # First, check if it already exists or if the user set force.
            if(not os.path.exists( Path(subs_dir + parentDir) )):
                os.makedirs( Path(subs_dir + parentDir) )

            if( not os.path.isfile(subs_dir + parentDir + '/' + name) or args.force_dl):
                print("Downloading: " + name)
                file = session.get(BASE_URL + page)
                open((subs_dir + parentDir + '/' + name), 'wb').write(file.content)
            else:
                print(name + "already exists. Skipping...")
        else:
            if( not os.path.isfile(subs_dir + name) or args.force_dl):
                print("Downloading: " + name)
                file = session.get(BASE_URL + page)
                open((subs_dir + name), 'wb').write(file.content)
            else:
                print(name + "already exists. Skipping...")

# Scrape main page
session = requests.Session()
scrape_main( (BASE_URL + TOPLEVEL_DIR) )
individual_pages = generate_arr()
subpage_list = fetch_subpages(individual_pages, session)

for path, name in subpage_list:
    new_arr = generate_arr( page = path )
    download_subs(new_arr, session, parentDir = name)

updateTxt = Path(subs_dir + "__updated.txt")
updateString = "Last updated on: " + datetime.datetime.now().strftime("%B %d, %Y, %H:%M:%S")

file = open(updateTxt, 'w')
file.write(updateString)
file.close()