from pathlib import Path
from bs4 import BeautifulSoup
import argparse
from datetime import date
import os
import requests

BASE_URL = 'https://kitsunekko.net/'
TOPLEVEL_DIR = 'dirlist.php?dir=subtitles%2Fjapanese%2F'
DEFAULT_TOPLEVEL_FILENAME = 'kitsunekko.html'

subs_dir = './data/'
temp_pages_dir = './pages/'
temp_subpages_dir = 'subpage/'

# Set up argument parser.
argParser = argparse.ArgumentParser(description='Scrapes Kitsunekko.net for Japanese subtitles.')

argParser.add_argument('--force-dl', 
    action = 'store_true',
    help = 'Whether or not to re-download files you already have.'
)

argParser.add_argument('--override-dir', 
    action = 'store', 
    type=str,
    help='Pass in a string to replace the default directory to download subtitles (default ./data).'
)

argParser.add_argument('--use-local', 
    action = 'store_true',
    help='Whether to fetch data from the remote page or from a local copy. Mostly for Debug purposes.'
)

args = argParser.parse_args()

if(args.override_dir):
    subs_dir = args.override_dir
    if(not subs_dir.endswith('/')):
        subs_dir = subs_dir + '/'
    print("User overrided directory: " + subs_dir)

## Scrapes the primary Kitsunekko page and writes it to a file.
## @param url: The url to scrape (kitsunekko.net).
def scrape_main(url):
    # Configure a temp file.
    mainPagePath = Path(temp_pages_dir + DEFAULT_TOPLEVEL_FILENAME)

    # If the user doesn't already have a pages dir, make it.
    if(not os.path.exists(Path(temp_pages_dir)) ):
        os.makedirs(Path(temp_pages_dir))

    # If the user doesn't have a pre-existing page to chew on, or they didn't set use-local, write it.
    if(not os.path.isfile(mainPagePath) or not args.use_local):
        print("Downloading the main kitsunekko.net page.")
        data = requests.get( (BASE_URL + TOPLEVEL_DIR) ) 
        data = data.text
        
        file = open(mainPagePath, "w")
        file.write(data)
        file.close()

## Scrapes a page passed in as a parameter and writes it to a file under the subPage directory.
## @param page: Page to scrape for data.
## @returns a constrcuted path to that page that was written to the disk.
def scrape_subpage(page):
    subPagePath = Path(temp_pages_dir + temp_subpages_dir + page + ".html")

    # Create the path if it doesn't exist already.
    if(not os.path.exists(temp_pages_dir + temp_subpages_dir)):
        os.makedirs(temp_pages_dir + temp_subpages_dir)

    # If the page we're looking to scrape hasn't been written already, or the user hasn't asked to use local files, then go download it.
    if(not os.path.isfile(subPagePath) or not args.use_local):
        data = requests.get( (BASE_URL + page) )
        data = data.text
        
        file = open(subPagePath, 'w')
        file.write(data)
        file.close()

    return subPagePath

## Given a page to eat, scrapes for the first table body found and creates a list of tuples for the href and the value.
## @param  page (optional) The filename of the page to read
## @returns A list of tuples, in the format (url stem, name of group) 
def generate_arr(*, page = DEFAULT_TOPLEVEL_FILENAME):
    # Open the DL'd or existing page to read.
    pagePath = Path(temp_pages_dir + page)
    file = open(pagePath, 'r')
    data = file.read()

    # Create our BS4 object and generate a list of individual pages.
    htmlParse = BeautifulSoup(data, 'html.parser')
    
    # The main page only has one table; We could make this more robust in the future.
    anitable = htmlParse.body.table.tbody
    links = anitable.find_all('a')
    individual_pages = []
    for link in links:
        individual_pages.append((link['href'], link.strong.text))

    return individual_pages

## Given a list of tuples generated by generate_arr, iterates though and downloads all attached pages and subs.
## If it finds a sub-page under the listed URL, it will recurse and pass that name down for the children to exist in that subfolder.
## @param urlList: a list of tuples in the format (url stem, name) from generate_arr().
## @param (optional) parentDir: the directory in which this file should live.
def parse_individual(urlList, *, parentDir = None):
    for page, name in urlList:
        # If the page url ends with %2F, we have another page to traverse.
        # Recurse down the page.
        if(page.endswith('%2F' or '%2f')):
            print("Found Page " + name)
            if(not os.path.exists(subs_dir + name)):
                os.makedirs(subs_dir + name)
            new_page = scrape_subpage(page)
            new_arr = generate_arr( page=(temp_subpages_dir + new_page.stem + ".html") )
            parse_individual(new_arr, parentDir = name)
        # Otherwise, we've found a file, so go ahead and download it.
        # TODO: A little smelly. I'd like to cut back on duplicate code later.
        # TODO: I'd also like to make these asynchronous so that we are not blocked by request slowly downloading.
        else:
            if(parentDir is not None):
                # First, check if it already exists or if the user set force.
                if( not os.path.isfile(subs_dir + parentDir + '/' + name) or args.force_dl):
                    print("Downloading: " + name)
                    file = requests.get(BASE_URL + page)
                    open((subs_dir + parentDir + '/' + name), 'wb').write(file.content)
                else:
                    print(name + "already exists. Skipping...")
            else:
                if( not os.path.isfile(subs_dir + name) or args.force_dl):
                    print("Downloading: " + name)
                    open((subs_dir + name), 'wb').write(file.content)
                else:
                    print(name + "already exists. Skipping...")

# Scrape main page
scrape_main( (BASE_URL + TOPLEVEL_DIR) )
individual_pages = generate_arr()
parse_individual(individual_pages)

updateTxt = Path(subs_dir + "__updated.txt")
updateString = "Last updated on: " + date.today()

file = open(updateTxt, 'w')
file.write(updateString)
file.close()